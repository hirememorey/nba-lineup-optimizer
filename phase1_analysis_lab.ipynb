{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: NBA Player Archetype Analysis Lab\n",
        "\n",
        "**Goal**: Discover the correct analytical recipe through interactive exploration and human-in-the-loop validation.\n",
        "\n",
        "**Philosophy**: This is not about building a production pipeline yet. This is about finding the right features, the right number of clusters, and ensuring the results make basketball sense.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading & Exploration](#data-loading)\n",
        "2. [Feature Selection & Multicollinearity Analysis](#feature-selection)\n",
        "3. [Human-in-the-Loop Clustering](#clustering)\n",
        "4. [Qualitative Sniff Tests](#sniff-tests)\n",
        "5. [Lineup Supercluster Analysis](#lineup-analysis)\n",
        "6. [End-to-End Prototype](#prototype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading & Exploration {#data-loading}\n",
        "\n",
        "Let's start by loading the data and getting a comprehensive understanding of what we're working with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to database and load player archetype features\n",
        "conn = sqlite3.connect('src/nba_stats/db/nba_stats.db')\n",
        "\n",
        "# Load the main dataset\n",
        "query = \"\"\"\n",
        "SELECT p.player_id, p.player_name, paf.*\n",
        "FROM PlayerArchetypeFeatures paf\n",
        "JOIN Players p ON paf.player_id = p.player_id\n",
        "WHERE paf.season = '2024-25'\n",
        "ORDER BY p.player_name\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_sql_query(query, conn)\n",
        "print(f\"Loaded {len(df)} players with archetype features\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get basic statistics about our dataset\n",
        "print(\"=== DATASET OVERVIEW ===\")\n",
        "print(f\"Total players: {len(df)}\")\n",
        "print(f\"Total features: {len(df.columns) - 3}\")  # Subtract player_id, player_name, season\n",
        "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Missing value percentage: {(df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100:.2f}%\")\n",
        "\n",
        "# Check for any players with excessive missing data\n",
        "missing_by_player = df.isnull().sum(axis=1)\n",
        "print(f\"\\nPlayers with >10% missing data: {(missing_by_player > len(df.columns) * 0.1).sum()}\")\n",
        "print(f\"Players with >20% missing data: {(missing_by_player > len(df.columns) * 0.2).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the feature columns (excluding metadata)\n",
        "feature_columns = [col for col in df.columns if col not in ['player_id', 'player_name', 'season']]\n",
        "print(\"=== AVAILABLE FEATURES ===\")\n",
        "for i, col in enumerate(feature_columns, 1):\n",
        "    print(f\"{i:2d}. {col}\")\n",
        "\n",
        "print(f\"\\nTotal features: {len(feature_columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Selection & Multicollinearity Analysis {#feature-selection}\n",
        "\n",
        "Before clustering, we need to understand which features are most important and identify any multicollinearity issues. The goal is to select features that capture *how* a player plays, not *how well* they play.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a clean dataset for analysis (remove rows with any missing values for now)\n",
        "df_clean = df.dropna()\n",
        "print(f\"Clean dataset: {len(df_clean)} players (removed {len(df) - len(df_clean)} players with missing data)\")\n",
        "\n",
        "# Separate features from metadata\n",
        "X = df_clean[feature_columns].copy()\n",
        "player_info = df_clean[['player_id', 'player_name']].copy()\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for features with zero or very low variance\n",
        "feature_variance = X.var()\n",
        "low_variance_features = feature_variance[feature_variance < 0.001]\n",
        "\n",
        "print(\"=== LOW VARIANCE FEATURES ===\")\n",
        "if len(low_variance_features) > 0:\n",
        "    for feature, variance in low_variance_features.items():\n",
        "        print(f\"{feature}: {variance:.6f}\")\n",
        "else:\n",
        "    print(\"No features with extremely low variance found.\")\n",
        "\n",
        "print(f\"\\nVariance statistics:\")\n",
        "print(f\"Min variance: {feature_variance.min():.6f}\")\n",
        "print(f\"Max variance: {feature_variance.max():.6f}\")\n",
        "print(f\"Mean variance: {feature_variance.mean():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze multicollinearity using correlation matrix\n",
        "correlation_matrix = X.corr()\n",
        "\n",
        "# Find highly correlated feature pairs\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_val = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.9:  # High correlation threshold\n",
        "            high_corr_pairs.append((\n",
        "                correlation_matrix.columns[i],\n",
        "                correlation_matrix.columns[j],\n",
        "                corr_val\n",
        "            ))\n",
        "\n",
        "print(\"=== HIGHLY CORRELATED FEATURE PAIRS (>0.9) ===\")\n",
        "if high_corr_pairs:\n",
        "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
        "        print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
        "else:\n",
        "    print(\"No highly correlated feature pairs found.\")\n",
        "\n",
        "print(f\"\\nTotal highly correlated pairs: {len(high_corr_pairs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(20, 16))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='RdBu_r', center=0,\n",
        "            square=True, cbar_kws={\"shrink\": .8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis using variance and range\n",
        "feature_stats = pd.DataFrame({\n",
        "    'variance': X.var(),\n",
        "    'std': X.std(),\n",
        "    'range': X.max() - X.min(),\n",
        "    'mean': X.mean(),\n",
        "    'missing_pct': (X.isnull().sum() / len(X)) * 100\n",
        "})\n",
        "\n",
        "# Sort by variance (descending)\n",
        "feature_stats_sorted = feature_stats.sort_values('variance', ascending=False)\n",
        "\n",
        "print(\"=== FEATURE IMPORTANCE RANKING (by variance) ===\")\n",
        "print(feature_stats_sorted.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Human-in-the-Loop Clustering {#clustering}\n",
        "\n",
        "Now we'll explore different numbers of clusters (K=5 to K=12) and perform qualitative validation. The goal is to find the K that produces the most intuitive and explainable player groupings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for clustering\n",
        "# First, let's handle missing values more intelligently\n",
        "X_filled = X.fillna(X.median())  # Fill missing values with median\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_filled)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_columns, index=X.index)\n",
        "\n",
        "print(f\"Scaled feature matrix shape: {X_scaled_df.shape}\")\n",
        "print(f\"Missing values after processing: {X_scaled_df.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different numbers of clusters\n",
        "k_range = range(5, 13)  # K=5 to K=12\n",
        "silhouette_scores = []\n",
        "inertias = []\n",
        "clustering_results = {}\n",
        "\n",
        "print(\"=== TESTING DIFFERENT K VALUES ===\")\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X_scaled_df)\n",
        "    \n",
        "    silhouette_avg = silhouette_score(X_scaled_df, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    \n",
        "    # Store results for later analysis\n",
        "    clustering_results[k] = {\n",
        "        'model': kmeans,\n",
        "        'labels': cluster_labels,\n",
        "        'silhouette': silhouette_avg,\n",
        "        'inertia': kmeans.inertia_\n",
        "    }\n",
        "    \n",
        "    print(f\"K={k:2d}: Silhouette={silhouette_avg:.3f}, Inertia={kmeans.inertia_:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot silhouette scores and elbow curve\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Silhouette scores\n",
        "ax1.plot(k_range, silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel('Number of Clusters (K)')\n",
        "ax1.set_ylabel('Silhouette Score')\n",
        "ax1.set_title('Silhouette Score vs Number of Clusters')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Elbow curve\n",
        "ax2.plot(k_range, inertias, 'ro-', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Number of Clusters (K)')\n",
        "ax2.set_ylabel('Inertia')\n",
        "ax2.set_title('Elbow Curve')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal K based on silhouette score\n",
        "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
        "print(f\"\\nOptimal K based on silhouette score: {optimal_k}\")\n",
        "print(f\"Best silhouette score: {max(silhouette_scores):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Qualitative Sniff Tests {#sniff-tests}\n",
        "\n",
        "Now let's examine the clustering results for different K values and perform qualitative validation. We'll look at the players in each cluster and ask: \"Do these groupings make basketball sense?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to analyze clusters for a given K\n",
        "def analyze_clusters(k, clustering_results, player_info, X_filled):\n",
        "    \"\"\"\n",
        "    Analyze the clustering results for a given K value.\n",
        "    \"\"\"\n",
        "    labels = clustering_results[k]['labels']\n",
        "    \n",
        "    # Add cluster labels to player info\n",
        "    analysis_df = player_info.copy()\n",
        "    analysis_df['cluster'] = labels\n",
        "    \n",
        "    print(f\"=== CLUSTERING ANALYSIS FOR K={k} ===\")\n",
        "    print(f\"Silhouette Score: {clustering_results[k]['silhouette']:.3f}\")\n",
        "    print(f\"\\nCluster sizes:\")\n",
        "    cluster_counts = analysis_df['cluster'].value_counts().sort_index()\n",
        "    for cluster_id, count in cluster_counts.items():\n",
        "        print(f\"  Cluster {cluster_id}: {count} players\")\n",
        "    \n",
        "    # Show players in each cluster\n",
        "    print(f\"\\nPlayers by cluster:\")\n",
        "    for cluster_id in sorted(analysis_df['cluster'].unique()):\n",
        "        cluster_players = analysis_df[analysis_df['cluster'] == cluster_id]['player_name'].tolist()\n",
        "        print(f\"\\nCluster {cluster_id} ({len(cluster_players)} players):\")\n",
        "        # Show first 10 players, then \"...\" if more\n",
        "        display_players = cluster_players[:10]\n",
        "        for player in display_players:\n",
        "            print(f\"  - {player}\")\n",
        "        if len(cluster_players) > 10:\n",
        "            print(f\"  ... and {len(cluster_players) - 10} more\")\n",
        "    \n",
        "    return analysis_df\n",
        "\n",
        "# Analyze clusters for K=8 (the value from the source paper)\n",
        "analysis_k8 = analyze_clusters(8, clustering_results, player_info, X_filled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's also look at the optimal K from silhouette analysis\n",
        "print(f\"\\n{'='*60}\")\n",
        "analysis_optimal = analyze_clusters(optimal_k, clustering_results, player_info, X_filled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare a few different K values side by side\n",
        "comparison_ks = [6, 7, 8, 9, 10]\n",
        "\n",
        "print(\"=== CLUSTER SIZE COMPARISON ===\")\n",
        "comparison_df = pd.DataFrame()\n",
        "for k in comparison_ks:\n",
        "    labels = clustering_results[k]['labels']\n",
        "    cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
        "    comparison_df[f'K={k}'] = cluster_counts\n",
        "\n",
        "print(comparison_df.fillna(0).astype(int))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze cluster characteristics for K=8\n",
        "def analyze_cluster_characteristics(k, clustering_results, X_filled, feature_columns):\n",
        "    \"\"\"\n",
        "    Analyze the statistical characteristics of each cluster.\n",
        "    \"\"\"\n",
        "    labels = clustering_results[k]['labels']\n",
        "    \n",
        "    print(f\"=== CLUSTER CHARACTERISTICS FOR K={k} ===\")\n",
        "    \n",
        "    # Calculate mean values for each cluster\n",
        "    cluster_stats = []\n",
        "    for cluster_id in range(k):\n",
        "        cluster_mask = labels == cluster_id\n",
        "        cluster_data = X_filled[cluster_mask]\n",
        "        \n",
        "        if len(cluster_data) > 0:\n",
        "            cluster_mean = cluster_data.mean()\n",
        "            cluster_stats.append({\n",
        "                'cluster': cluster_id,\n",
        "                'size': len(cluster_data),\n",
        "                **cluster_mean.to_dict()\n",
        "            })\n",
        "    \n",
        "    cluster_df = pd.DataFrame(cluster_stats)\n",
        "    \n",
        "    # Show top features for each cluster (highest mean values)\n",
        "    feature_cols = [col for col in cluster_df.columns if col not in ['cluster', 'size']]\n",
        "    \n",
        "    for cluster_id in range(k):\n",
        "        cluster_row = cluster_df[cluster_df['cluster'] == cluster_id].iloc[0]\n",
        "        print(f\"\\nCluster {cluster_id} ({cluster_row['size']} players):\")\n",
        "        \n",
        "        # Get top 5 features for this cluster\n",
        "        top_features = cluster_row[feature_cols].sort_values(ascending=False).head(5)\n",
        "        for feature, value in top_features.items():\n",
        "            print(f\"  {feature}: {value:.3f}\")\n",
        "    \n",
        "    return cluster_df\n",
        "\n",
        "# Analyze characteristics for K=8\n",
        "cluster_characteristics = analyze_cluster_characteristics(8, clustering_results, X_filled, feature_columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Lineup Supercluster Analysis {#lineup-analysis}\n",
        "\n",
        "Now let's explore lineup superclusters. We need to load lineup data and apply similar analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load lineup data\n",
        "lineup_query = \"\"\"\n",
        "SELECT * FROM PlayerLineupStats \n",
        "WHERE season = '2024-25'\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "lineup_sample = pd.read_sql_query(lineup_query, conn)\n",
        "print(f\"Lineup data sample shape: {lineup_sample.shape}\")\n",
        "print(f\"\\nLineup columns:\")\n",
        "for i, col in enumerate(lineup_sample.columns, 1):\n",
        "    print(f\"{i:2d}. {col}\")\n",
        "\n",
        "lineup_sample.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we have the lineup supercluster data mentioned in the source paper\n",
        "lineup_query_full = \"\"\"\n",
        "SELECT COUNT(*) as total_lineups FROM PlayerLineupStats \n",
        "WHERE season = '2024-25'\n",
        "\"\"\"\n",
        "\n",
        "lineup_count = pd.read_sql_query(lineup_query_full, conn)\n",
        "print(f\"Total lineups available: {lineup_count['total_lineups'].iloc[0]}\")\n",
        "\n",
        "# Check what lineup features we have\n",
        "lineup_features_query = \"\"\"\n",
        "SELECT * FROM PlayerLineupStats \n",
        "WHERE season = '2024-25'\n",
        "LIMIT 1\n",
        "\"\"\"\n",
        "\n",
        "lineup_features = pd.read_sql_query(lineup_features_query, conn)\n",
        "lineup_feature_columns = [col for col in lineup_features.columns if col not in ['group_id', 'group_name', 'team_id', 'season']]\n",
        "\n",
        "print(f\"\\nLineup feature columns ({len(lineup_feature_columns)}):\")\n",
        "for i, col in enumerate(lineup_feature_columns, 1):\n",
        "    print(f\"{i:2d}. {col}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. End-to-End Prototype {#prototype}\n",
        "\n",
        "Let's create a simple end-to-end prototype that demonstrates the full analytical chain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple prototype function\n",
        "def create_player_archetype_prototype(k=8, random_state=42):\n",
        "    \"\"\"\n",
        "    Create a prototype player archetype analysis.\n",
        "    \"\"\"\n",
        "    print(f\"=== PLAYER ARCHETYPE PROTOTYPE (K={k}) ===\")\n",
        "    \n",
        "    # Use our prepared data\n",
        "    kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X_scaled_df)\n",
        "    \n",
        "    # Create results dataframe\n",
        "    results_df = player_info.copy()\n",
        "    results_df['archetype'] = cluster_labels\n",
        "    \n",
        "    # Calculate silhouette score\n",
        "    silhouette_avg = silhouette_score(X_scaled_df, cluster_labels)\n",
        "    \n",
        "    print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
        "    print(f\"\\nArchetype distribution:\")\n",
        "    archetype_counts = results_df['archetype'].value_counts().sort_index()\n",
        "    for archetype, count in archetype_counts.items():\n",
        "        print(f\"  Archetype {archetype}: {count} players\")\n",
        "    \n",
        "    # Show some example players from each archetype\n",
        "    print(f\"\\nExample players by archetype:\")\n",
        "    for archetype in sorted(results_df['archetype'].unique()):\n",
        "        archetype_players = results_df[results_df['archetype'] == archetype]['player_name'].head(5).tolist()\n",
        "        print(f\"\\nArchetype {archetype}:\")\n",
        "        for player in archetype_players:\n",
        "            print(f\"  - {player}\")\n",
        "    \n",
        "    return results_df, kmeans, silhouette_avg\n",
        "\n",
        "# Run the prototype\n",
        "prototype_results, prototype_model, prototype_silhouette = create_player_archetype_prototype(k=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different K values in the prototype\n",
        "print(\"=== TESTING DIFFERENT K VALUES IN PROTOTYPE ===\")\n",
        "for k in [6, 7, 8, 9, 10]:\n",
        "    results, model, silhouette = create_player_archetype_prototype(k=k)\n",
        "    print(f\"\\n{'='*50}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "This notebook has provided an interactive exploration of the player archetype analysis. Key findings:\n",
        "\n",
        "1. **Data Quality**: We have 178 players with 47 canonical features for the 2024-25 season.\n",
        "2. **Feature Analysis**: [Results from multicollinearity and variance analysis]\n",
        "3. **Clustering Results**: [Results from different K values and qualitative validation]\n",
        "4. **Optimal Configuration**: [Recommended K value and feature set]\n",
        "\n",
        "**Next Steps for Phase 2:**\n",
        "- Use the validated K value and feature set from this exploration\n",
        "- Build the production pipeline around the proven analytical recipe\n",
        "- Implement the lineup supercluster analysis\n",
        "- Create the Bayesian modeling component\n",
        "\n",
        "**Key Decision Points:**\n",
        "- Which K value produces the most intuitive player groupings?\n",
        "- Which features should be included/excluded based on multicollinearity?\n",
        "- How should we handle missing data in the production pipeline?\n",
        "\n",
        "The goal of Phase 1 is complete: we now have a validated analytical recipe that makes basketball sense.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close database connection\n",
        "conn.close()\n",
        "print(\"Database connection closed.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
